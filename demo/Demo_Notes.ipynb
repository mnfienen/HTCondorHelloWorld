{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Hello cruel world demo of HTCondor\n",
    "This uses a simple Python script to demonstrate a general HTCondor submit file. Of course, the intent is that the trivial example be replaced with meaningful work!\n",
    "\n",
    "The general idea is that in many applications, it is useful to run a sequence of executions, each tagged with a number that they can use as input, internalize, and use to tag their output. To get going, make a simple Python script that accepts a single value as input, prints that value to a file and also to `stdout`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script `cruel.py`\n",
    "The function `cruel` accepts a single number as input and prints both to a file and to the screen. It also sleeps for five seconds just to simulate that any executable should take a little time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "# define a little hello cruel world function\n",
    "def cruel(run_number):\n",
    "    # print out a simple statement to a file and include the run number\n",
    "    with open('outfile{0}.dat'.format(run_number), 'w') as ofp:\n",
    "        outmsg = 'Hello cruel world! I am but a number and it is {0}'.format(run_number)\n",
    "        ofp.write(outmsg)\n",
    "\n",
    "    # write some stuff to the screen as well just for fun\n",
    "    print ('Hi there, from number {0}\\n'.format(run_number))\n",
    "    print ('Waiting for 5 long seconds')\n",
    "    for i in range(5):\n",
    "        sys.stdout.write(u'.')\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1)\n",
    "    print ('\\n')\n",
    "    return outmsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it with argument `99` and see the output\n",
    "Note that is printed to `stdout` and also wrote a file called `outfile99.dat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, from number 99\n",
      "\n",
      "Waiting for 5 long seconds\n",
      ".....\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello cruel world! I am but a number and it is 99'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cruel(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Parallel on local machine\n",
    "This simple example acn be run for a variety of input values. Since each one is independent, this is \"embarassingly\" or \"pleasingly\" parallel. So, we should be able to run it in parallel without needing to modify the function `cruel` at all. The simplest way is using the `multiprocessing` function in Python.\n",
    "\n",
    "###script `parallel_cruel.py` in `HolaCruelWorld/demo_parallel_local`\n",
    "\n",
    "To run this script, simply enter `python parallel_cruel.py 0 99 8 2 4` where the space-delimited sequence of numbers are values for which the function `cruel` will be run. The code below is modified from `parallel_cruel.py` slightly to run in this directory. Better to run from the command line, though, as it is a little clunky within the iPython notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, from number 99\n",
      "Hi there, from number 0\n",
      "Hi there, from number 8\n",
      "Hi there, from number 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Waiting for 5 long seconds\n",
      "Waiting for 5 long seconds\n",
      "Waiting for 5 long seconds\n",
      "Waiting for 5 long seconds\n",
      "....................\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hi there, from number 4\n",
      "\n",
      "Waiting for 5 long seconds\n",
      ".....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the hello cruel world function\n",
    "# from cruel import cruel --> don't need to import since \n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "\n",
    "# read in the arguments passed from command line \n",
    "# args = [int(i) for i in sys.argv[1:]] --> just specify in place to run in the notebook\n",
    "args = [0, 99, 8, 2, 4]\n",
    "# set up a pool with 4 cores working\n",
    "pool = Pool(4)\n",
    "\n",
    "# run all the arguments\n",
    "results = pool.map(cruel, args)\n",
    "\n",
    "# now write out the results to a file\n",
    "with open('alldata.dat', 'w') as ofp:\n",
    "    for cresult in results:\n",
    "        ofp.write('{0}\\n'.format(cresult))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel on HTCondor --> simplest case\n",
    "In the directory `HolaCruelWorld` are both a submit file `test0.sub` and the executable it will call `worker0.sh`\n",
    "\n",
    "### `test0.sub`\n",
    "\n",
    "Important notes:\n",
    " * `notification=Never` is __VITAL__ to avoid getting an email for every run!\n",
    " * The executable is `worker0.sh` with the argument being the sequential number assigned to each run (`$(Process)`) and this is passed on to the `cruel` function.\n",
    " * No information given on transferring output, so all new files generated on each worker will be returned to the submitting directory.\n",
    " * The `queue` argument specifies how many workers to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "notification=Never\n",
    "universe = vanilla\n",
    "log = log/worker_$(Cluster).log\n",
    "output = log/worker_$(Cluster)_$(Process).out\n",
    "error = log/worker_$(Cluster)_$(Process).err\n",
    "executable = worker0.sh\n",
    "stream_output = True\n",
    "stream_error = True\n",
    "arguments = $(Process)\n",
    "requirements =((Target.OpSys==\"LINUX\")  && (Target.Arch==\"X86_64\"))\n",
    "request_memory = 500\n",
    "when_to_transfer_output = ON_EXIT\n",
    "should_transfer_files = yes\n",
    "transfer_input_files =  cruel.py\n",
    "queue 2 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `worker0.sh`\n",
    "Very simple shell script that accepts the run sequential number as input and passes it on to the `cruel` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#!/bin/sh\n",
    "\n",
    "# grab the number passed in from the master\n",
    "run_number=$1\n",
    "# run python script with the number passed in\n",
    "python cruel.py $run_number\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Test this out!\n",
    "\n",
    "Simply type `condor_status` to make sure resources are available. Output should look like:\n",
    "\n",
    "```\n",
    "Name                               OpSys      Arch   State     Activity LoadAv Mem  ActivityTime\n",
    "\n",
    "slot1@igsarmewfsm000.xx.xxx.edu   LINUX      X86_64 Unclaimed Idle      0.070 2460  10+01:41:19\n",
    "slot2@igsarmewfsm000.xx.xxx.edu   LINUX      X86_64 Unclaimed Idle      0.000 2460  10+01:41:52\n",
    "... ... ... ...\n",
    "slot1@IGSARMEWWSM46.xx.xxx.edu     WINDOWS    X86_64 Unclaimed Idle      0.570 32573   9+14:39:53\n",
    "slot1@IGSARMEWWSM47.xx.xxx.edu     WINDOWS    X86_64 Unclaimed Idle      0.620 32708   3+00:34:42\n",
    "slot1@IGSARMEWWSM48.xx.xxx.edu     WINDOWS    X86_64 Unclaimed Idle      0.600 32708   3+00:34:44\n",
    "                     Machines Owner Claimed Unclaimed Matched Preempting\n",
    "\n",
    "        X86_64/LINUX       44     0       0        44       0          0\n",
    "      X86_64/WINDOWS       41     0       0        41       0          0\n",
    "\n",
    "               Total       85     0       0        85       0          0\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Then `condor_submit test0.sub` will produce:\n",
    "```\n",
    "Submitting job(s)..\n",
    "2 job(s) submitted to cluster 16.\n",
    "```\n",
    "\n",
    "`condor_q` will show what is running:\n",
    "```\n",
    "-- Submitter: licon31.xx.xxx.edu : <###.##.###.##:####> : licon31.xx.xxx.edu\n",
    " ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               \n",
    "  17.0   someperson      8/13 16:46   0+00:00:01 R  0   0.0  worker0.sh 0      \n",
    "  17.1   someperson      8/13 16:46   0+00:00:01 R  0   0.0  worker0.sh 1      \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Parallel on HTCondor --> better data management case\n",
    "In the directory `HolaCruelWorld` are both a submit file `test0.sub` and the executable it will call `worker0.sh`\n",
    "\n",
    "###`test1.sub`\n",
    "\n",
    "Important notes:\n",
    " * `notification=Never` is __VITAL__ to avoid getting an email for every run!\n",
    " * The executable is now `worker1.sh` which uncompresses the `data.tar` file (in this case only contains `cruel.py` but typically would include all executables, dependencies, and data needed for each worker (unless `worker1.sh` is designed to pull data from a cache location, FTP, etc.)\n",
    " * This time, no files are generated in the base run directory because runs are performed in `data` directory (see `worker1.sh`). So, specific files are requested and the `remap` moves them into a `results` subdirectory on the submit machine.\n",
    " * You will need to tar up the data folder using `tar czf data.tar data` in the submit directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "notification=Never\n",
    "universe = vanilla\n",
    "log = log/worker_$(Cluster).log\n",
    "output = log/worker_$(Cluster)_$(Process).out\n",
    "error = log/worker_$(Cluster)_$(Process).err\n",
    "executable = worker1.sh\n",
    "stream_output = True\n",
    "stream_error = True\n",
    "arguments = $(Process)\n",
    "requirements =((Target.OpSys==\"LINUX\")  && (Target.Arch==\"X86_64\"))\n",
    "request_memory = 500\n",
    "when_to_transfer_output = ON_EXIT\n",
    "should_transfer_files = yes\n",
    "transfer_output_files = data/outfile$(Process).dat\n",
    "transfer_output_remaps = \"outfile$(Process).dat = results/outfile$(Process).dat\"\n",
    "transfer_input_files =data.tar\n",
    "queue 1000  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###`worker1.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#!/bin/sh\n",
    "\n",
    "# grab the number passed in from the master\n",
    "run_number=$1\n",
    "\n",
    "# keeping our focus\n",
    "SqUiRReL!!!!!!!!!!!\n",
    "\n",
    "# uncompress the data folder and jump into it\n",
    "tar xzf data.tar\n",
    "cd data\n",
    "\n",
    "# run python script with the number passed in\n",
    "python cruel.py $run_number\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this the same way -- `condor_submit test1.sub` and look at results in the `results` directory. Also, can run `consol_data.py` to dump all the results output files into a single file `alldata.dat`. This would, in reality, be a more sophisticated postprocessing routine, of course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Wrapup\n",
    "Note with `test0.sub` what happens in the `log` directory. The main log file shows how the cluster ran overall (HTCondor's information). The `<file>.out` and `<file>.err` return standard out and standard error from each worker. What's different with `<file>.err` for `test1.sub`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
